+++
title = 'OpenAI-o1'
date = 2024-09-11T20:02:20+08:00
draft = false
+++

> GPT 4o的问题在于本身大模型的智力水平还不够高，所以做不了复杂任务，导致很多应用场景无法实用化，而指望靠图片、视频这类新模态数据大幅提升大模型智力水平是不太可能的，尽管确实能拓展更丰富的多模态应用场景，但这类数据弥补的更多是大模型对外在多模态世界的感知能力，而不是认知能力。

4o的智力水平实际上也在提升，比如ChatGPT-4o-latest (2024-08-08)的推理明显要比GPT-4o-2024-05-13强很多。 
而1o有点像是instruct with CoT datasets, 专长推理。可能作为未来多模态模型中的推理base模型，base reasoning model -> text, image, audio

> OpenAI o1的做法本质上是COT的自动化。
> 从用户提出的问题形成树的根结点出发，最终走到给出正确答案，可以想像成类似AlphaGo下棋，形成了巨大的由COT具体步骤构成的树形搜索空间，这里COT的具体步骤的组合空间是巨大的，人写的COT未必最优。如果我们有大量逻辑数据，是由<问题，明确的正确答案>构成，则通过类似AlphaGo的Monte Carlo Tree Search（MCTS）搜索+强化学习，确实是可以训练大模型快速找到通向正确答案的COT路径的。

这里的想法和Plan Search中类似，首先生成可能的推理节点，每一个节点可能包含对于细分任务的思维链中的步骤。而使用广度搜索来进行任务的泛化，也即是路径依赖。
其中一个问题是，这里的CoT是从哪来。如果是类似于人工标注后的RLHF，那么必然对任务的广度有所局限。

> 如果通过基座模型Plan把一个复杂任务分解为10个步骤，哪怕单个步骤的正确率高达95%，要想最后把任务做对，10个环节的准确率连乘下来，最终的正确率只有59%，惨不忍睹。

实际的问题，但是COT和拆分成类似langgraph的node还是有所区别。不是简单的独立事件。思维链更像是narrow down的过程，将事情缩小到指定的集合中做分类或者回归。这里的描述更像是多个独立的分类问题。

> o1的Model Card专门测试了Agent任务，对于简单和中等难度的Agent任务有明显提升，但是复杂的、环节多的任务准确率还是不太高。

这里的应该代表了o1内在的流程，divide&conquer问题。如果在一个思维链中无法解决时，需要拆分成多个。因此简单问题在一个思维链中提升因为基准性能的提升，而多个思维链需要考虑到概率的乘数。

> 大语言模型最基础的能力有三种：语言理解和表达能力、世界知识存储和查询能力以及逻辑推理能力（包括数学、Coding、推理等理科能力，这里Coding有一定的特殊性，是语言能力和逻辑掺杂在一起的混合能力，Coding从语言角度可以看成一种受限的自然语言，但是混杂着复杂的内在逻辑问题。从语言角度看，Coding貌似是容易解决的，从逻辑角度看又相对难解决。总之，Coding目前看是除了语言理解外，大模型做得最好的方向）

逻辑推理能力是哲学逻辑中的内核，是思维本身。
语言表达能力是思维的出口，而上层的表达可以是NLP，Audio或者是Video。
世界知识存储和查询能力是代表了模型的一个是压缩能力，还有一个是本身的回归能力。

> 但幻觉问题目前无法根治，这是制约各种应用的硬伤之一

这和我之前的假设有所出入，当然实现起来是MLP本身的掣肘，比如dropout带来的信息丢失。但幻觉应该是可以通过调节attention中的位置，比如Q-KV匹配时，让Q和KV的相关性尽可能最大化。
我们能够从数学上的提升得到一些收获，比如数学处理的概率问题。数据问题的优势在于它的线性和可复现，因此可以通过不断的迭代和微调来提高。
而更广阔的问题因为没有足够的ground truth，会导致模型无法学到真正核心的eureka point.

> 而为啥逻辑推理能力最难提升？因为能体现这方面的自然数据（代码、数学题、物理题、科学论文等）在训练数据中比例太低，自然大模型就学不好，尽管通过不断增加数据，能增加逻辑推理方面数据的绝对数量，但因为占比太少，这方面提升的效果和增加的总体数据规模就不成比例，效果也不会太明显，就体现在逻辑推理能力Scaling law看上去的放缓。这是很自然的。这也是为何现在为了提高模型逻辑能力，往往在预训练阶段和Post-training阶段，大幅增加逻辑推理数据占比的原因，且是有成效的。

Model card: https://assets.ctfassets.net/kftzwdyauwt9/67qJD51Aur3eIc96iOfeOP/71551c3d223cd97e591aa89567306912/o1_system_card.pdf

https://mp.weixin.qq.com/s/FXGdJA8OyZvLl89rXJiyAQ

> 技术要点有三：

> 后训练扩展律 Post-Training Scaling Laws 已经出现，并且 Post-Training Scaling Laws 为上述技术路径的成功提供了有力支持。

> 模型学习的是产生合理推理的过程，MCTS 在其中的作用是诱导合理推理过程的产生或构建相应的偏序对形成细粒度奖励信号，而非直接搜索过程和最终答案。

> 模型的 BootStrap 有助于构建新的高质量数据，并且新的 Rationales 数据促进了模型进一步提升能力。

RLHF实际上也是商用模型能和开源的基准模型拉开差距的地方，展示了财力和算力。

第三点中的bootstrap指的是模型通过自我迭代来生成新的高质量训练数据的过程。具体来说：

1. 模型首先使用现有的训练数据进行训练
2. 然后利用训练好的模型来生成新的数据，特别是推理过程(Rationales)
3. 这些新生成的高质量数据被加入到训练集中
4. 模型使用扩充后的数据集重新训练，进一步提升能力
5. 重复这个过程，不断改进模型性能

这种方法允许模型逐步提升自身的推理能力，特别是在逻辑推理等难以获得大量高质量训练数据的领域。通过不断生成和利用新的Rationales数据，模型可以持续优化其推理过程和结果质量。

> 自回归模型在数学推理问题上很难进步的一点在于没有办法进行回答的自主修正，如果仅是依靠生成式方法和扩大参数规模，那么在数学推理任务上带来的收益不会太大。所以需要寻找额外的 Scaling Laws。

